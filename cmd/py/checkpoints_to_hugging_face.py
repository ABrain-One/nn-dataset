import sys
import os

# --- AUTO-PATH SETUP (Make script Plug & Play) ---
# Automatically detects project root and adds it to system path.
# This ensures the script runs without manually setting 'export PYTHONPATH'.
try:
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.abspath(os.path.join(current_dir, "../../"))
    if project_root not in sys.path:
        sys.path.append(project_root)
    print(f"âœ… Project Root detected at: {project_root}")
except Exception as e:
    print(f"âš ï¸ Warning: Auto-path setup failed: {e}")

# --- STANDARD IMPORTS ---
import json
import shutil
import time
from pathlib import Path
from huggingface_hub import HfApi, hf_hub_download

# --- PROJECT IMPORTS ---
try:
    from ab.nn.api import data
    from ab.nn.train import main as train_main
    from ab.nn.util.Const import stat_train_dir, ckpt_dir
    from ab.nn.util.Util import release_memory
except ImportError:
    print("\nâŒ Critical Error: Could not import 'ab.nn' modules.")
    print("   Please ensure you are running this script from the project root or 'cmd/py' folder.")
    sys.exit(1)

# --- CONFIGURATION ---
HF_TOKEN = os.environ.get("HF_TOKEN")
if not HF_TOKEN:
    # Fallback for testing
    HF_TOKEN = ""
    print("âš ï¸ Warning: No HF_TOKEN found. Please set environment variable.")
HF_USERNAME = "NN-Dataset"
SUMMARY_FILENAME = "all_models_summary.json"

# =================================================================
# âš ï¸ SETTINGS
# =================================================================
TEST_MODE = False
TEST_LIMIT = 10


# =================================================================

def get_existing_models_and_summary(repo_id):
    """
    Downloads the Master JSON from Hugging Face.
    """
    print("â˜ï¸ Fetching Master Summary from Hugging Face...")

    summary_data = {}
    uploaded_models = set()

    try:
        local_path = hf_hub_download(
            repo_id=repo_id,
            filename=SUMMARY_FILENAME,
            token=HF_TOKEN,
            local_dir="."
        )
        with open(local_path, 'r') as f:
            summary_data = json.load(f)

        # Note: We are not deleting the local file so it can be inspected if needed.

        uploaded_models = set(summary_data.keys())
        print(f"âœ… Found Master JSON with {len(uploaded_models)} records.")

    except Exception as e:
        print(f"âš ï¸ Master JSON not found (Starting fresh): {e}")

    return uploaded_models, summary_data


def train_and_save(model_name, params, epoch_max):
    print(f"ðŸš€ Starting FRESH training for {model_name}...")
    config_pattern = f"img-classification_cifar-10_acc_{model_name}"
    try:
        release_memory()
        train_main(
            config=config_pattern,
            nn_prm=params,
            epoch_max=epoch_max,
            n_optuna_trials=-1,
            save_pth_weights=True,
            save_onnx_weights=False,
            train_missing_pipelines=False,
            num_workers=0
        )
        print(f"âœ… Training completed call for {model_name}")
        return True
    except Exception as e:
        print(f"âŒ Training failed/crashed for {model_name}: {e}")
        return False


def get_metadata_from_stats(model_name, epoch_max, dataset, task, metric):
    """
    Extracts the latest training statistics (accuracy, time, etc.) from the stats directory.
    """
    try:
        config_name = f"img-classification_cifar-10_acc_{model_name}"
        stat_path = stat_train_dir / config_name
        if not stat_path.exists(): return None

        json_files = list(stat_path.glob("*.json"))
        if not json_files: return None
        latest_json = max(json_files, key=os.path.getmtime)

        with open(latest_json, 'r') as f:
            data = json.load(f)

        stats = {}
        if isinstance(data, list) and data:
            stats = data[-1]
        elif isinstance(data, dict):
            stats = data
        if not stats: return None

        return {
            "nn": model_name,
            "accuracy": stats['accuracy'],
            "epoch": stats.get('epoch', epoch_max),
            "duration": stats['duration'],
            "dataset": dataset,
            "task": task,
            "metric": metric,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        }
    except:
        return None


def upload_to_hf(model_name, epoch_max, dataset, task, metric, summary_data, repo_id):
    print(f"â˜ï¸ Uploading {model_name} to Hugging Face...")

    # --- SUPER FIX: BLIND SEARCH ---
    # Strategy: Since we clean the output folder before training, 
    # we don't need to match the filename. We simply grab the newest .pth file found.
    expected_file = None

    # Check 1: ckpt_dir (standard path)
    files_in_ckpt = list(ckpt_dir.rglob("*.pth"))
    # Check 2: 'out/checkpoints' (Manual fallback)
    files_in_out = list(Path("out/checkpoints").rglob("*.pth"))
    # Check 3: Current directory recursive (Last resort)
    files_in_curr = list(Path(".").rglob("*.pth"))

    # Combine results
    all_found = files_in_ckpt + files_in_out + files_in_curr

    # Filter: Remove .venv files and duplicates
    valid_files = list(set([f for f in all_found if ".venv" not in str(f) and "site-packages" not in str(f)]))

    if valid_files:
        # Pick the most recent file (Created within the last minute)
        latest_file = max(valid_files, key=os.path.getmtime)
        print(f"   ðŸ” Super-Search found latest file: {latest_file}")
        expected_file = latest_file
    else:
        # Green warning (No panic) - likely low accuracy model
        print("   â„¹ï¸ No .pth file generated (Likely due to low accuracy). Uploading Metadata only.")

    api = HfApi(token=HF_TOKEN)
    try:
        api.create_repo(repo_id=repo_id, repo_type="model", exist_ok=True)

        # 1. Upload .pth (If exists)
        if expected_file and expected_file.exists():
            api.upload_file(
                path_or_fileobj=str(expected_file),
                path_in_repo=f"{model_name}/{model_name}.pth",
                repo_id=repo_id,
                repo_type="model"
            )

        # 2. Update Master Data
        new_metadata = get_metadata_from_stats(model_name, epoch_max, dataset, task, metric)
        if new_metadata:
            summary_data[model_name] = new_metadata

            # Save locally (NO DELETE)
            with open(SUMMARY_FILENAME, 'w') as f:
                json.dump(summary_data, f, indent=4)

            # 3. Upload Master JSON
            api.upload_file(
                path_or_fileobj=SUMMARY_FILENAME,
                path_in_repo=SUMMARY_FILENAME,
                repo_id=repo_id,
                repo_type="model"
            )

        print(f"âœ… Successfully processed {model_name}")

        print("â³ Waiting 30s to avoid Rate Limit...")
        time.sleep(30)

        return True
    except Exception as e:
        print(f"âŒ Upload failed: {e}")
        if "429" in str(e):
            print("ðŸ›‘ Hit Rate Limit! Waiting 2 minutes...")
            time.sleep(120)
        return False


def main():
    try:
        print("ðŸ“Š Fetching models from API...")
        epoch_max = 5
        dataset = 'cifar-10'
        task = 'img-classification'
        metric = 'acc'
        REPO_NAME = "checkpoints-epoch-" + str(epoch_max)
        repo_id = f"{HF_USERNAME}/{REPO_NAME}"

        df = (data(only_best_accuracy=True, task=task, dataset=dataset, metric=metric, epoch=epoch_max,
                   nn_prefixes=('rag-', 'unq-'))
              .sort_values(by='accuracy', ascending=False))

        if TEST_MODE:
            df = df[:TEST_LIMIT]
            print(f"âš ï¸ TEST MODE: Only running first {TEST_LIMIT} models.")

        uploaded_models, summary_data = get_existing_models_and_summary(repo_id)

    except Exception as e:
        print(f"âŒ Error initializing: {e}")
        return

    print(f"ðŸ”¥ Starting Pipeline for {len(df)} models...")

    for i, dt in df.iterrows():
        print(f"\n{'=' * 60}")
        model = dt['nn']
        print(f"Processing Model: {i}/{len(df)} | {model}")

        if model in uploaded_models:
            print(f"â­ï¸ Skipping {model} (Found in Master JSON)")
            continue

        params = dt['prm']

        if 'batch' in params:
            if params['batch'] > 32:
                print(f"   ðŸ“‰ Reducing Batch Size from {params['batch']} to 32 for safety.")
                params['batch'] = 32
        else:
            params['batch'] = 32

        # Local cleanup per model to ensure fresh state
        if os.path.isdir(ckpt_dir):
            try:
                shutil.rmtree(ckpt_dir)
            except:
                pass

        if os.path.isdir("out/checkpoints"):
            try:
                shutil.rmtree("out/checkpoints")
            except:
                pass

        success = train_and_save(model, params, epoch_max)
        if success:
            if upload_to_hf(model, epoch_max, dataset, task, metric, summary_data, repo_id):
                uploaded_models.add(model)

        release_memory()


if __name__ == "__main__":
    main()
